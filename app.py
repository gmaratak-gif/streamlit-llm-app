# app.py
# Streamlit Ã— LangChain Ã— OpenAI ç°¡æ˜“LLMã‚¢ãƒ—ãƒª

import os
from typing import Literal

import streamlit as st
from dotenv import load_dotenv

# === ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ ===
load_dotenv()

# Streamlit Cloud ã® Secrets ã‚’å„ªå…ˆ
if hasattr(st, "secrets") and "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# === LangChain ã®æº–å‚™ ===
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«
ExpertKey = Literal["æ ªå¼æŠ•è³‡ã‚¢ãƒŠãƒªã‚¹ãƒˆï¼ˆAï¼‰", "ç”ŸæˆAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆBï¼‰"]

EXPERT_SYSTEM_MESSAGES: dict[ExpertKey, str] = {
    "æ ªå¼æŠ•è³‡ã‚¢ãƒŠãƒªã‚¹ãƒˆï¼ˆAï¼‰": (
        "ã‚ãªãŸã¯å³æ ¼ã§å®¢è¦³çš„ãªæ ªå¼æŠ•è³‡ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"
        "å¸¸ã«æ ¹æ‹ ãƒ»å‰æãƒ»ãƒªã‚¹ã‚¯ãƒ»ä»£æ›¿æ¡ˆã‚’æç¤ºã—ã€"
        "å°‚é–€ç”¨èªã¯çŸ­ãèª¬æ˜ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚"
        "æ¨å¥¨ã¯æ–­å®šã›ãšã€æ•™è‚²ç›®çš„ã¨ã—ã¦è¿°ã¹ã¾ã™ã€‚"
    ),
    "ç”ŸæˆAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆBï¼‰": (
        "ã‚ãªãŸã¯å®Ÿå‹™ã«å¼·ã„ç”ŸæˆAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚"
        "è¨­è¨ˆæ–¹é‡â†’æ‰‹é †â†’ã‚³ãƒ¼ãƒ‰ä¾‹â†’æ¤œè¨¼æ–¹æ³•â†’é‹ç”¨ä¸Šã®æ³¨æ„ã®é †ã§ã€"
        "å…·ä½“çš„ã‹ã¤å†ç¾å¯èƒ½ãªæ‰‹é †ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    ),
}


def build_chain(expert_key: ExpertKey, model_name: str = "gpt-4o-mini", temperature: float = 0.2):
    """é¸æŠã—ãŸå°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã§ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰"""
    system_msg = EXPERT_SYSTEM_MESSAGES[expert_key]

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_msg),
            ("human", "{user_input}"),
        ]
    )

    llm = ChatOpenAI(model=model_name, temperature=temperature)
    parser = StrOutputParser()
    return prompt | llm | parser


def get_llm_answer(user_text: str, expert_key: ExpertKey, model_name: str = "gpt-4o-mini", temperature: float = 0.2) -> str:
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ­ãƒ¼ãƒ«ã‚’åŸºã«LLMã®å›ç­”ã‚’è¿”ã™"""
    chain = build_chain(expert_key=expert_key, model_name=model_name, temperature=temperature)
    return chain.invoke({"user_input": user_text})


# === Streamlit UI ===
st.set_page_config(page_title="Streamlit Ã— LangChain LLM ã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–", layout="centered")

st.title("ğŸ¤– Streamlit Ã— LangChain LLM ã‚¢ãƒ—ãƒª")
st.caption("Python 3.11 / LangChain / OpenAI APIï¼ˆ.env ã¾ãŸã¯ Secretsï¼‰")

with st.expander("â„¹ï¸ ä½¿ã„æ–¹", expanded=True):
    st.markdown(
        """
        - å…¥åŠ›æ¬„ã«è³ªå•ã‚’å…¥åŠ›ã—ã€é€ä¿¡ã™ã‚‹ã¨LLMãŒå›ç­”ã—ã¾ã™ã€‚  
        - ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã®å½¹å‰²ã‚’é¸ã¶ã¨ã€å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚  
        """
    )

# APIã‚­ãƒ¼æœªè¨­å®šã®è­¦å‘Š
if not OPENAI_API_KEY:
    st.warning("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ Streamlit Secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚", icon="âš ï¸")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼‰
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    model_name = st.selectbox("ãƒ¢ãƒ‡ãƒ«", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    temperature = st.slider("Temperatureï¼ˆå‰µé€ æ€§ï¼‰", 0.0, 1.0, 0.2, 0.1)
    st.markdown("---")
    st.markdown("**æ³¨æ„**: Streamlit Cloud ã§ã¯ Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ 3.11 ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«é¸æŠ
expert_choice: ExpertKey = st.radio(
    "å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š",
    options=list(EXPERT_SYSTEM_MESSAGES.keys()),
    horizontal=True,
)

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
with st.form(key="llm_form", clear_on_submit=False):
    user_text = st.text_area(
        "è³ªå• / ãŠé¡Œ",
        placeholder="ä¾‹ï¼‰ã“ã®ãƒ†ãƒ¼ãƒã§æŠ•è³‡ã®ãƒªã‚¹ã‚¯è¦å› ã‚’æ•´ç†ã—ã¦ã€å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦ãã ã•ã„â€¦",
        height=140,
    )
    submitted = st.form_submit_button("é€ä¿¡")

# é€ä¿¡å‡¦ç†
if submitted:
    if not OPENAI_API_KEY:
        st.error("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚.env ã‚‚ã—ãã¯ Streamlit Secrets ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    elif not user_text.strip():
        st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("LLM ãŒè€ƒãˆã¦ã„ã¾ã™â€¦"):
            try:
                answer = get_llm_answer(
                    user_text=user_text.strip(),
                    expert_key=expert_choice,
                    model_name=model_name,
                    temperature=temperature,
                )
                st.success("å›ç­”ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
                st.markdown("### ğŸ“ å›ç­”")
                st.markdown(answer)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

